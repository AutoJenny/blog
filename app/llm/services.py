"""Service for interacting with LLM providers."""

import httpx
import logging
from flask import current_app
from datetime import datetime
from jinja2 import Template
import psycopg2
import psycopg2.extras

logger = logging.getLogger(__name__)
# All ORM model imports removed. Use direct SQL via psycopg2 for any DB access.

def execute_llm_request(request_data):
    """Execute an LLM request with the given parameters."""
    try:
        service = LLMService()
        
        # Log the incoming request
        logger.info(f"Executing LLM request with data: {request_data}")
        
        # Get the prompt and input
        prompt_template = request_data.get('prompt', '').strip()
        input_text = request_data.get('input', '').strip()
        
        # Format the prompt in a clear, explicit way
        prompt = f"""Instructions: {prompt_template}

Input Topic: {input_text}

Please provide your response based on the above input topic, following the instructions:"""

        logger.debug(f"Using formatted prompt: {prompt}")
        
        # Generate the output
        output = service.generate(
            prompt,
            model_name=request_data['model_name'],
            temperature=request_data.get('temperature', 0.7),
            max_tokens=request_data.get('max_tokens', 2000)
        )
        
        if not output:
            logger.error("LLM service returned empty output")
            return {'error': 'No output generated by the LLM service'}
            
        logger.info(f"Successfully generated output")
        
        # Return in the format expected by the frontend
        return {
            'model_used': request_data['model_name'],
            'response': output
        }
        
    except Exception as e:
        logger.exception(f"Error executing LLM request: {str(e)}")
        return {'error': str(e)}

def assemble_prompt_from_parts(action, fields: dict):
    """
    Assemble a prompt or message list from all prompt parts for an action, ordered.
    For OpenAI: returns a list of {role, content} dicts.
    For Ollama: returns a single concatenated string.
    """
    prompt_parts = (
        LLMActionPromptPart.query.filter_by(action_id=action.id)
        .order_by(LLMActionPromptPart.order)
        .all()
    )
    messages = []
    for part_link in prompt_parts:
        part = part_link.prompt_part
        # Render with Jinja2
        try:
            content = Template(part.content).render(**fields)
        except Exception as e:
            logger.error(f"Error rendering prompt part {part.id}: {e}")
            content = part.content
        # For OpenAI, use role; for Ollama, just concatenate
        if part.type in ('system', 'user', 'assistant'):
            messages.append({'role': part.type, 'content': content})
        else:
            # For style/instructions/other, treat as user message
            messages.append({'role': 'user', 'content': content})
    # For Ollama, concatenate all content
    ollama_prompt = '\n\n'.join([m['content'] for m in messages])
    return {'openai': messages, 'ollama': ollama_prompt}

class LLMService:
    """Service for interacting with LLM providers."""

    def __init__(self):
        """Initialize the LLM service."""
        self.config = None  # provider type (e.g., 'ollama', 'openai')
        self.api_url = None  # provider API URL

    def generate(self, prompt, model_name=None, temperature=0.7, max_tokens=1000):
        """Generate text using configured LLM, supporting both OpenAI (messages) and Ollama (string)."""
        if not model_name:
            model_name = self.config
        logger.info(f"Generating with model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")
        if self.config == "ollama":
            return self._generate_ollama(prompt, model_name, temperature, max_tokens)
        elif self.config == "openai":
            return self._generate_openai(prompt, model_name, temperature, max_tokens)
        else:
            raise ValueError(f"Unsupported provider type: {self.config}")

    def _generate_ollama(self, prompt, model_name, temperature=0.7, max_tokens=1000):
        """Generate text using Ollama."""
        try:
            request_data = {
                "model": model_name,
                "prompt": prompt,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False  # Ensure we get a complete response
            }
            logger.debug(f"Sending request to Ollama: {request_data}")
            if not self.api_url:
                raise ValueError("Ollama API URL not set on LLMService")
            response = httpx.post(
                f"{self.api_url}/api/generate",
                json=request_data,
                timeout=180.0,
            )
            response.raise_for_status()
            response_data = response.json()
            logger.debug(f"Received response from Ollama: {response_data}")
            # Handle both response formats
            if "response" in response_data:
                return response_data["response"]
            elif isinstance(response_data, dict) and "model_used" in response_data and "response" in response_data:
                return response_data["response"]
            else:
                logger.error(f"Unexpected response format from Ollama: {response_data}")
                raise ValueError("Unexpected response format from Ollama")
        except httpx.TimeoutError:
            logger.error(f"Timeout while generating with Ollama (model: {model_name})")
            raise TimeoutError("Request to Ollama timed out")
        except httpx.HTTPError as e:
            logger.error(f"HTTP error while generating with Ollama: {str(e)}")
            raise
        except Exception as e:
            logger.exception(f"Error generating with Ollama: {str(e)}")
            raise

    def _generate_openai(self, prompt, model_name, temperature=0.7, max_tokens=1000):
        """Generate text using OpenAI."""
        try:
            import openai
            client = openai.OpenAI(api_key=current_app.config["OPENAI_API_KEY"])
            response = client.chat.completions.create(
                model=model_name,
                messages=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error generating with OpenAI: {str(e)}")
            raise

    def execute_action(self, action, fields: dict, post_id=None, model_name=None):
        # --- PATCH: Resolve model/provider from registry ---
        model = None
        provider = None
        # Use model_name if provided, else resolve from action.llm_model
        if model_name:
            model = model_name
            # Try to resolve provider from model name
            model_obj = LLMModel.query.filter(LLMModel.name == model_name).first()
            if not model_obj:
                raise ValueError(f"LLM model '{model_name}' not found in registry")
            provider = LLMProvider.query.get(model_obj.provider_id)
        elif action.llm_model:
            # PATCH: Use .name or .id if action.llm_model is an object
            if isinstance(action.llm_model, LLMModel):
                model_obj = action.llm_model
            elif isinstance(action.llm_model, str):
                model_obj = LLMModel.query.filter(LLMModel.name == action.llm_model).first()
            elif isinstance(action.llm_model, int):
                model_obj = LLMModel.query.filter(LLMModel.id == action.llm_model).first()
            else:
                raise ValueError(f"Invalid type for action.llm_model: {type(action.llm_model)}")
            if not model_obj:
                raise ValueError(f"LLM model '{action.llm_model}' not found in registry")
            model = model_obj.name  # Use string only
            provider = LLMProvider.query.get(model_obj.provider_id)
        else:
            raise ValueError(f"LLM model not set on action {action.id}")
        if not provider:
            raise ValueError(f"LLM provider for model '{model}' not found in registry")
        # Set config and api_url for routing
        provider_type = provider.name.lower() if provider.name else None
        self.config = provider_type  # 'ollama' or 'openai'
        self.api_url = provider.api_url
        # Assemble prompt/messages from modular prompt parts
        prompt_parts = (
            LLMActionPromptPart.query.filter_by(action_id=action.id)
            .order_by(LLMActionPromptPart.order)
            .all()
        )
        messages = []
        for part_link in prompt_parts:
            part = part_link.prompt_part
            # Render with Jinja2
            try:
                content = Template(part.content).render(**fields)
            except Exception as e:
                logger.error(f"Error rendering prompt part {part.id}: {e}")
                content = part.content
            # For OpenAI, use role; for Ollama, just concatenate
            if part.type in ('system', 'user', 'assistant'):
                messages.append({'role': part.type, 'content': content})
            else:
                # For style/instructions/other, treat as user message
                messages.append({'role': 'user', 'content': content})
        ollama_prompt = '\n\n'.join([m['content'] for m in messages])
        # Choose input/output fields
        input_field = getattr(action, 'input_field', None) or 'input'
        output_field = getattr(action, 'output_field', None) or 'output'
        # Call LLM (OpenAI or Ollama)
        if provider_type == 'openai':
            # OpenAI: use messages
            result = self.generate(messages, model_name=model, temperature=action.temperature, max_tokens=action.max_tokens)
        elif provider_type == 'ollama':
            # Ollama: use prompt string
            result = self.generate(ollama_prompt, model_name=model, temperature=action.temperature, max_tokens=action.max_tokens)
        else:
            raise ValueError(f"Unsupported provider type: {provider_type}")
        # Map output to output_field
        if isinstance(result, dict) and 'output' in result:
            result = {output_field: result['output'], **{k: v for k, v in result.items() if k != 'output'}}
        elif isinstance(result, str):
            result = {output_field: result}
        # At the end, before returning result:
        def make_json_safe(obj):
            if isinstance(obj, db.Model):
                if hasattr(obj, 'to_dict'):
                    return obj.to_dict()
                return str(obj)
            elif isinstance(obj, dict):
                return {k: make_json_safe(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [make_json_safe(i) for i in obj]
            return obj
        return make_json_safe(result)

# Remove all ORM model usage below, stub out as needed for migration. 