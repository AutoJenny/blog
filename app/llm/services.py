"""Service for interacting with LLM providers."""

import httpx
import logging
from flask import current_app
from app.models import LLMConfig, LLMInteraction
from app import db
from datetime import datetime

logger = logging.getLogger(__name__)

def execute_llm_request(request_data):
    """Execute an LLM request with the given parameters."""
    try:
        service = LLMService()
        
        # Log the incoming request
        logger.info(f"Executing LLM request with data: {request_data}")
        
        # Get the prompt and input
        prompt_template = request_data.get('prompt', '').strip()
        input_text = request_data.get('input', '').strip()
        
        # Format the prompt in a clear, explicit way
        prompt = f"""Instructions: {prompt_template}

Input Topic: {input_text}

Please provide your response based on the above input topic, following the instructions:"""

        logger.debug(f"Using formatted prompt: {prompt}")
        
        # Generate the output
        output = service.generate(
            prompt,
            model_name=request_data['model_name'],
            temperature=request_data.get('temperature', 0.7),
            max_tokens=request_data.get('max_tokens', 2000)
        )
        
        if not output:
            logger.error("LLM service returned empty output")
            return {'error': 'No output generated by the LLM service'}
            
        logger.info(f"Successfully generated output")
        
        # Return in the format expected by the frontend
        return {
            'model_used': request_data['model_name'],
            'response': output
        }
        
    except Exception as e:
        logger.exception(f"Error executing LLM request: {str(e)}")
        return {'error': str(e)}

class LLMService:
    """Service for interacting with LLM providers."""

    def __init__(self):
        """Initialize the LLM service."""
        self.config = LLMConfig.query.first()
        if not self.config:
            self.config = LLMConfig(
                provider_type="ollama",
                model_name="mistral",
                api_base="http://localhost:11434",
            )

    def generate(self, prompt, model_name=None, temperature=0.7, max_tokens=1000):
        """Generate text using configured LLM, optionally specifying a model name."""
        if not model_name:
            model_name = self.config.model_name
            
        logger.info(f"Generating with model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")

        if self.config.provider_type == "ollama":
            return self._generate_ollama(prompt, model_name, temperature, max_tokens)
        elif self.config.provider_type == "openai":
            return self._generate_openai(prompt, model_name, temperature, max_tokens)
        else:
            raise ValueError(f"Unsupported provider type: {self.config.provider_type}")

    def _generate_ollama(self, prompt, model_name, temperature=0.7, max_tokens=1000):
        """Generate text using Ollama."""
        try:
            request_data = {
                "model": model_name,
                "prompt": prompt,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False  # Ensure we get a complete response
            }
            logger.debug(f"Sending request to Ollama: {request_data}")
            
            response = httpx.post(
                f"{self.config.api_base}/api/generate",
                json=request_data,
                timeout=60.0,
            )
            response.raise_for_status()
            
            response_data = response.json()
            logger.debug(f"Received response from Ollama: {response_data}")
            
            # Handle both response formats
            if "response" in response_data:
                return response_data["response"]
            elif isinstance(response_data, dict) and "model_used" in response_data and "response" in response_data:
                return response_data["response"]
            else:
                logger.error(f"Unexpected response format from Ollama: {response_data}")
                raise ValueError("Unexpected response format from Ollama")
            
        except httpx.TimeoutError:
            logger.error(f"Timeout while generating with Ollama (model: {model_name})")
            raise TimeoutError("Request to Ollama timed out")
        except httpx.HTTPError as e:
            logger.error(f"HTTP error while generating with Ollama: {str(e)}")
            raise
        except Exception as e:
            logger.exception(f"Error generating with Ollama: {str(e)}")
            raise

    def _generate_openai(self, prompt, model_name, temperature=0.7, max_tokens=1000):
        """Generate text using OpenAI."""
        try:
            import openai
            client = openai.OpenAI(api_key=current_app.config["OPENAI_API_KEY"])
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error generating with OpenAI: {str(e)}")
            raise

    def execute_action(self, action, post_id, input_text):
        """Execute an LLM action on a post field."""
        try:
            # Format the prompt template with the input
            prompt = action.prompt_template.replace("{{input}}", input_text)
            
            # Create history record
            history = LLMActionHistory(
                action_id=action.id,
                post_id=post_id,
                input_text=input_text,
                status="pending"
            )
            db.session.add(history)
            db.session.commit()
            
            try:
                # Generate the output
                output = self.generate(
                    prompt,
                    model_name=action.llm_model,
                    temperature=action.temperature,
                    max_tokens=action.max_tokens
                )
                
                # Update history record
                history.output_text = output
                history.status = "success"
                db.session.commit()
                
                return output
            except Exception as e:
                # Update history record with error
                history.status = "error"
                history.error_message = str(e)
                db.session.commit()
                raise
        except Exception as e:
            logger.error(f"Error executing action: {str(e)}")
            raise 